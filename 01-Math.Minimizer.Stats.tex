\include{header}

% Main title, author etc. in header
\subtitle{Part I -- Math and Minimizers}

\begin{document}
  \maketitle

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MATH
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \section{Math}

% vectors and matrices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Vectors and Matrices}

% <Vectors>
  \begin{frame}{Vectors}
    \begin{itemize}
      \item Vectors have both magnitude ($|\vec{v}| \geq 0$) and direction
        \rightarrow \enquote{arrow}
      \item Vectors are represented by components indicating their length
        along the basis directions
      \item Usually we write column vectors,
        \enquote{rotate} a vector up to get the transposed version
        \begin{equation*}
          \left(v_1, v_2, \dots, v_n\right)^\mathrm{T} \coloneqq
          \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
        \end{equation*}
      \item Vectors can be added by putting the end of one vector at the tip
        of another one. Equivalent to summing each individual components
        \begin{equation*}
          \vec{v} + \vec{w} =
            \left(v_1 + w_1, v_2 + w_2, \dots, v_n + w_n\right)^\mathrm{T}
        \end{equation*}
    \end{itemize}
  \end{frame}

  \begin{frame}{Vectors}
    \begin{itemize}
      \item Scalar multiplication:
        Multiply every component with the scalar $\alpha$
        \begin{equation*}
          \alpha \vec{v} =
            \alpha \left(v_1, v_2, \dots, v_n\right)^\mathrm{T} =
            \left(\alpha v_1, \alpha v_2, \dots, \alpha v_n\right)^\mathrm{T}
        \end{equation*}
      \item Norm (length) of a vector
        \begin{equation*}
          |\vec{v}| = \sqrt{\vec{v}\cdot \vec{v}} =
            \sqrt{v_1^2 + v_2^2 + \dots + v_n^2}
        \end{equation*}
      \item Scalar product (also \enquote{inner product}):
        Multiply each element and sum up, produces a single number $\beta$
        \begin{equation*}
          \vec{v}\cdot \vec{w} =
            v_1 w_1 + v_2 w_2 + \dots + v_n w_n =
            \sum_{i=1}^{N} v_i w_i = \beta
        \end{equation*}
        Alternatively the following form is used to get the angle $\phi$
          between two vectors
        \begin{equation*}
          \vec{v}\cdot \vec{w} \coloneqq |\vec{v}||\vec{w}|\cos(\phi)
        \end{equation*}

    \end{itemize}
  \end{frame}
% </Vectors>

% <Vector Exercise>
  \begin{frame}{Vectors}
    \begin{exampleblock}{Exercise}
      We have two 4D vectors
      \begin{equation*}
        \vec{v} = \left(1, -2, 5, 0\right)^\mathrm{T}
        \quad\mathrm{and}\quad
        \vec{w} = \left(2, 2, 3, -1\right)^\mathrm{T}
      \end{equation*}
      \begin{enumerate}
        \item Calculate the lengths $|\vec{v}|$, $|\vec{w}|$
        \item Calculate the sum $\vec{v} + \vec{w}$ and difference $\vec{v} - \vec{w}$
        \item Calculate the scalar product $\vec{v}\cdot \vec{w}$
        \item What is angle $\phi$ in degrees between both vectors?
        \item Find a vector $\vec{u}\neq \vec{0}$ which is orthogonal to $\vec{v}$
      \end{enumerate}
      The \texttt{numpy} package in Python has built-in support for vector
      calculations.
      Use \texttt{numpy.dot}, \texttt{numpy.linalg.norm},
      \texttt{numpy.rad2deg}, etc. to check your calculations above or code the
      functionality yourself.
      See \url{https://numpy.org/devdocs/reference/index.html} for help.
    \end{exampleblock}
  \end{frame}
% </Vector Exercise>

% <Matrices>
  % Intro, vector identification
  \begin{frame}{Matrices}
    \begin{itemize}
      \item Matrices are 2D collections of numbers
      \item A $(n, m)$ matrix $\underline{M}$ is defined as a tuple of numbers
        $a_{ij}$ ($i=1,\dots,m$, $j=1,\dots,n$)
        \begin{equation*}
          \underline{M} =
          \begin{pmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
          \end{pmatrix}
        \end{equation*}
      \item We can identify column vectors with $(m, 1)$ matrices and row
        vectors with $(1, n)$ matrices
        \begin{gather*}
          \underline{M}_{1,n} = \left(v_1, \dots, v_n\right)
            \leftrightarrow \vec{v}^\mathrm{T} \\
          \underline{M}_{m, 1} =
            \begin{pmatrix}w_1 \\ \vdots \\ w_m \end{pmatrix}
            \leftrightarrow \vec{w}
        \end{gather*}
    \end{itemize}
  \end{frame}

  % Scalar multiplication, matrix product
  \begin{frame}{Matrices}
    \begin{itemize}
      \item Multiplication with a scalar $\alpha$ is done by multiplication of
        every element with that scalar
        \begin{equation*}
          \alpha \underline{M} =
          \begin{pmatrix}
            \alpha a_{11} & \alpha a_{12} & \dots & \alpha a_{1n} \\
            \alpha a_{21} & \alpha a_{22} & \dots & \alpha a_{2n} \\
            \vdots & \vdots & & \vdots \\
            \alpha a_{m1} & \alpha a_{m2} & \dots & \alpha a_{mn} \\
          \end{pmatrix}
        \end{equation*}
      \item Matrix products $\underline{C} = \underline{A}\,\underline{B}$ are
        defined component-wise by
        \begin{equation*}
          c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
        \end{equation*}
        where $\underline{A}$ is a $(m, n)$, $\underline{B}$ a $(n, p)$ and
        $\underline{C}$ a $(m, p)$ matrix
        \begin{itemize}
          \item This is usually called
            \enquote{multiply row $i$ of $\underline{A}$ by
                     column $j$ of $\underline{B}$}
          % \item The inner index range must match, we can multiply any 2 matrices
          %   where the number of rows of the first on equals the number of columns
          %   in the second one
        \end{itemize}
      \begin{exampleblock}{Example: Matrix times vector}
        \begin{equation*}
          \begin{pmatrix}
            a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
          \end{pmatrix}_{(3,2)}
            \begin{pmatrix}v_1 \\ v_2\end{pmatrix}_{(2,1)} =
          \begin{pmatrix}
            a_{11}v_1 + a_{12}v_2 \\
            a_{21}v_1 + a_{22}v_2 \\
            a_{31}v_1 + a_{32}v_2
          \end{pmatrix}_{(3,1)}
        \end{equation*}
      \end{exampleblock}
    \end{itemize}
  \end{frame}

  % Transpose, inverse
  \begin{frame}{Matrices}
    \begin{itemize}
      \item The transpose of a matrix is obtained by switching
        elements $a_{ij}\leftrightarrow a_{ji}$
        \begin{equation*}
          \underline{M}^\mathrm{T} =
          \begin{pmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots &  & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
          \end{pmatrix}^\mathrm{T} =
          \begin{pmatrix}
            a_{11} & a_{21} & \dots & a_{m1} \\
            a_{12} & a_{22} & \dots & a_{m2} \\
            \vdots & \vdots &  & \vdots \\
            a_{1n} & a_{2n} & \dots & a_{mn} \\
          \end{pmatrix}
        \end{equation*}
      \item The inverse $\underline{M}^{-1}$ of a matrix is defined so that
        \begin{equation*}
          \underline{M}^{-1}\underline{M} =
          \underline{M}\,\underline{M}^{-1} = \underline{1}
        \end{equation*}
        where $\underline{1}$ is the identity matrix
        ($1$ on diagonal, $0$ everywhere else)
        \begin{itemize}
          \item The inverse can be found by solving a set of linear equations
            using the definition above
          \item Note: Not every matrix has an inverse!
          \item For $(2,2)$ matrices the inverse can be explicitly calculated by
            \begin{equation}
              \begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1} =
              \frac{1}{ad-bc}
                \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
            \end{equation}
        \end{itemize}
    \end{itemize}
  \end{frame}
% </Matrices>

% <Matrix Exercises>
  \begin{frame}{Matrices}
    \begin{exampleblock}{Exercise}
      \begin{enumerate}
        \item We have 2 matrices $\underline{A}, \underline{B}$
          and a vector $\vec{v}$
          \begin{equation*}
            \underline{A} =
              \begin{pmatrix} 1 & 2 & 3 \\ 3 & 2 & 1 \end{pmatrix}
            \,\mathrm{, }\quad
            \underline{B} =
              \begin{pmatrix} 0 & 2 \\ 1 & -1 \\ 0 & 1 \end{pmatrix}
            \,\mathrm{, }\quad
            \vec{v} = \begin{pmatrix} 4 \\ 5 \\ 6 \end{pmatrix}
          \end{equation*}
          Calculate $\underline{A}\,\underline{B}$,
          $\underline{B}\,\underline{A}$,
          $\underline{A}\,\vec{v}$ and $\vec{v}^\mathrm{T}\,\underline{B}$.
        \item Calculate the inverses (if existing) of
          \begin{equation*}
            \underline{A} =
              \begin{pmatrix} 1 & 2 \\ 4 & 2 \end{pmatrix}
            \quad\mathrm{, }\quad
            \underline{B} =
              \begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix}
          \end{equation*}
        \item We have a matrix $\underline{M} = \underline{1}$
          and a vector $\vec{v}^\mathrm{T} = \left(v_1, v_2, v_3\right)$.
          Calculate
          \begin{equation*}
            \vec{v}^\mathrm{T}\, \underline{M}\, \vec{v}
          \end{equation*}
      \end{enumerate}
    Again, use \texttt{numpy} or own code to check your results.
    \end{exampleblock}
  \end{frame}
% </Matrix Exercises>

% <Convolution>
  \begin{frame}{Convolution}
    \begin{itemize}
      \item Mathematically
        \begin{equation*}
          \left(f * g\right)(t) \coloneqq
            \int_{-\infty}^\infty f(\tau)g(t-\tau)\deriv{\tau}
        \end{equation*}
      \begin{itemize}
        \item Intuitive: $f$ is a \enquote{signal}, $g$ is a \enquote{response}.
          At time $t$ the convolution gives the (causal, thus we mirror $g$)
          system response for input $f$ and response $g$ by summing (integrating)
          all infinitesimal mini-responses up to $t$
          \item Imagine $f$ being decomposed in \enquote{delta} peaks and $g$
            acts independently on each input impulse
      \end{itemize}
      \item Here we need the discrete version, replacing integrals with sums
      and functions with arrays of numbers ($f(x)\rightarrow f[i]$)
      \begin{equation*}
        (f*g)[i] = \sum_{k=-\infty}^{\infty} f[k]g[i-k]
      \end{equation*}
      \item Note: In real application the kernel or the signal has some finite support
      (there are no infinite length arrays), so the sum will be constrained by
      some range $\{-N, -N+1, \dots, N-1, N\}$.
      \begin{itemize}
        \item Finite support leads to problems at the borders of $f$
        \item Multiple strategies exist, eg. padding with zero,
          mirroring at the border, cyclic repetition or
          repeating the outermost element
      \end{itemize}
    \end{itemize}
  \end{frame}
% </Convolution>

% <Convolution>
  \begin{frame}{Convolution}
    \begin{itemize}
      \item In two dimensions, convolutions work the same way
        \begin{equation*}
          (f*g)[i, j] =
          \sum_{k=-\infty}^{\infty} \sum_{l=-\infty}^{\infty} f[k,l]g[i-k, j-l]
        \end{equation*}
      \item Same here with the finite support
      \item In image applications, the response is often called kernel or filter
      \item Don't forget to flip the kernel horizontally and vertically before
        computing the elementwise multiplication
      \item This is what is used in convolutional neural network layers (although
        technically they don't flip the kernel, but the weights are learned anyway)
    \end{itemize}
  \end{frame}
% </Convolution>

% <Convolution Exercises>
  \begin{frame}{Convolution}
    \begin{exampleblock}{Exercise}
    For the following exercises, pad the signal function with zeroes where
    appropriate.
    % Additionally you can also try cyclic, mirrored or \enquote{repeated} edges or only calculate the convolution where the signal is valid.
      \begin{enumerate}
        \item Consider a discrete signal
          \begin{equation*}
            f[i] = \{f[i=0] = 3 \}
          \end{equation*}
          (value of $3$ at position $i=0$) and a response
          \begin{equation*}
            g[i] = \left\{g[i=0] = 2, g[i=1] = 1\right\}
          \end{equation*}
          Compute the convolution $f*g$ for all non-zero components.
        \item Now we have a more complicated signal
          \begin{equation*}
            f[i] = \left\{f[i=0] = 3, f[i=1] = 4, f[i=2] = 5\right\}
          \end{equation*}
          Compute $f*g$ with the same kernel $g$ as above. \\
          {\footnotesize Tip: \emph{For grasping the concept of why the kernel is flipped, try
          to imagine how each signal contribution triggers the response. The
          response needs some time to arrive at the current index which in the
          end gives you the flipped kernel in the formula.}}
      \end{enumerate}
    \end{exampleblock}
  \end{frame}
% </Convolution Exercises>

% <Convolution Exercises>
  \begin{frame}{Convolution}
    \begin{exampleblock}{Exercise}
      \begin{enumerate}
        \item[3] Now we try a 2D convolution.
          Convolve the following $8x8$ "grayscale image"
          \begin{equation*}
            \mathrm{Img}_1 = \quad \begin{matrix}
                \ccb \tcw{0.0} & \ccga 0.5 & \ccga 0.5 & \ccga 0.5 & \ccga 0.5 & \ccga 0.5 & \ccga 0.5 & \ccb \tcw{0.0} \\
                \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccga 0.5 & \ccga 0.5 & \ccga 0.5 & \ccga 0.5 & \ccb \tcw{0.0} & \ccb \tcw{0.0} \\
                \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} \\
                \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} \\
                \ccb \tcw{0.0} & 1.0 & 1.0 & \ccb \tcw{0.0} & \ccb \tcw{0.0} & 1.0 & 1.0 & \ccb \tcw{0.0} \\
                \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccb \tcw{0.0} \\
                \ccb \tcw{0.0} & \ccb \tcw{0.0} & \ccgc 0.7 & \ccgb 0.6 & \ccgb 0.6 & \ccgc 0.7 & \ccb \tcw{0.0} & \ccb \tcw{0.0} \\
                \ccga 0.5 & \ccb \tcw{0.0} & \ccgc 0.7 & \ccgc 0.7 & \ccgc 0.7 & \ccgc 0.7 & \ccb \tcw{0.0} & \ccga 0.5
            \end{matrix}
          \end{equation*}
          with a $3x3$ top to bottom Sobel kernel (gradient kernel for edge finding):
          \begin{equation*}
            \begin{pmatrix}
              1 & 2 & 1 \\
              0 & 0 & 0 \\
              -1 & -2 & -1
            \end{pmatrix}
          \end{equation*}
        Interpret the results.
      \end{enumerate}
    \end{exampleblock}
  \end{frame}
% </Convolution Exercises>

% functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Functions}

% <Functions>
  \begin{frame}{Functions}
    \begin{itemize}
      \item Functions define a mapping from one set to another
      \item Typically we deal with functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$ mapping from many to a single real number or $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ mapping from many to many real numbers (vector functions)
      \item Loss functions can have from a few to millions of input parameters and usually map to a single number
      \item In neural networks, the connections are built using vector functions
      \item Functions can be composed by applying each mapping in order, eg. for two functions $f, g$
      \begin{equation*}
        (g\circ f)(x) = g(f(x))
      \end{equation*}
      which means applying first $f$ on $x$ and then $g$ on the result of $f(x)$
    \end{itemize}
  \end{frame}
% </Functions>

% <Derivatives>
  \begin{frame}{Derivatives}
    \begin{itemize}
      \item We often need to know how fast or slow a function changes at a specific point, this is computed using the derivative
      \begin{equation*}
        \dderiv{f}{x} = \lim\limits_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h}
      \end{equation*}
      \item For functions of many parameters, the partial derivative is the derivative of the function considering only a single parameter
      \begin{equation*}
        \ddel{f}{x_i} = \lim\limits_{h\rightarrow 0}
          \frac{f(x_0, \dots, x_i+h, \dots) - f(\vec{x})}{h}
      \end{equation*}
    \item For vector functions the principle is the same, but with respect to each component of the vector
    \end{itemize}
  \end{frame}
% </Derivatives>

% <Derivatives>
  \begin{frame}{Derivatives}
    \begin{itemize}
      \item There are rules to derive many \enquote{standard} function elements like
      \begin{equation*}
        \dderiv{x^a}{x} = ax^{a-1} ,\quad \dderiv{e^x}{x} = e^x ,\quad \dderiv{\ln x}{x} = \frac{1}{x} ,\quad \dderiv{\sin x}{x} = \cos{x} ,\quad \dderiv{\cos x}{x} = -\sin x
      \end{equation*}
      \item Additionally there are rules that can be used when multiple functions are combined in a special arrangement ($f' \leftrightarrow \dderiv{f}{x}$)
      \begin{align*}
        (f + g)' &= f' + g' &&\quad\mathrm{sum\,rule} \\
        (f \cdot g)' &= f' \cdot g + f \cdot g' &&\quad\mathrm{product\,rule} \\
        (g \circ f)' &= g'(f) \cdot f' &&\quad\mathrm{chain\,rule}
      \end{align*}
      \item The chain rule is excessively used in the back-propagation algorithm to compute the gradient of a neural network
      \item The same differentiation rules hold for many parameter and vector functions \textbf{but the order matters, because we are dealing with vectors and matrices instead of single numbers}
    \end{itemize}
  \end{frame}
% </Derivatives>

% <Vector Functions>
  \begin{frame}{$\mathbf{R}^n$ and Vector Functions}
    \begin{itemize}
      \item For functions with more than one input parameter we already saw the partial derivative
      \item The gradient collects all possible derivatives in a new vector pointing along the direction of steepest ascent for functions $f:\mathbf{R}^n\rightarrow \mathbf{R}$
      \begin{equation*}
        \vec{\nabla}_{\vec{x}} f = \left[
          \ddel{f}{x_1}, \ddel{f}{x_2}, \dots, \ddel{f}{x_n} \right]_{(1, n)}
      \end{equation*}
      \item Equivalently the Jacobi matrix collects all gradients for a vector function $f:\mathbf{R}^n\rightarrow\mathbf{R}^m$ in a $(m, n)$ matrix
      \begin{equation*}
        \underline{J} = \vec{\nabla}_{\vec{x}} \vec{f} = \left[
          \ddel{\vec{f}}{x_1}, \ddel{\vec{f}}{x_2}, \dots, \ddel{\vec{f}}{x_n} \right] =
          \begin{pmatrix}
            \ddel{f_1}{x_1} & \dots & \ddel{f_1}{x_n} \\
            \vdots & & \vdots \\
            \ddel{f_m}{x_1} & \dots & \ddel{f_m}{x_n}
          \end{pmatrix}_{(m, n)}
      \end{equation*}
    \end{itemize}
  \end{frame}
% </Vector Functions>

% <Exercises>
  \begin{frame}{Exercises}
    \begin{exampleblock}{Exercise}
      \begin{enumerate}
        \item 1
      \end{enumerate}
    \end{exampleblock}
  \end{frame}
% </Exercises>

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% STATISTICS
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \section{Statistics}
% <Probabilities>
  \begin{frame}{Probabilities}
    \begin{itemize}
      \item Continuous distribution of a random variable $X$
        \begin{equation*}
          f(x) \quad\mathrm{with}\quad \int_\Omega f(x) \deriv{x} = 1
        \end{equation*}
        and $f(x) \geq 0$
      \item Discrete distribution of a random variable $X$
        \begin{equation*}
          P(x_i) \quad\mathrm{with}\quad \sum_{x_i\in\chi} P(x_i) = 1
        \end{equation*}
        and $P(x_i) \geq 0$
      \item Cumulative distribution functions define the probability contained up to a specific interval:
      \begin{equation*}
        F(x) = \int_{\chi_0}^x f(x) \deriv{x}
        \quad \mathrm{and} \quad
        F(n) = \sum_{i=i_\mathrm{min}}^n P(x_i)
      \end{equation*}
      for the continuous and the discrete (only when there is an explicit order) case respectively
    \end{itemize}
  \end{frame}
% </Probabilities>

% <Mean and Variance>
  \begin{frame}{Mean and Variance}
    \begin{itemize}
      \item The mean $<x>$ of a distribution $f(x)$ is defined as
        \begin{equation*}
          \langle x\rangle = \int_\chi xf(x) \deriv{x}
          \quad\mathrm{or}\quad
          \langle x\rangle = \sum_{x_i\in\chi} x_iP(x_i)
        \end{equation*}
      \item In general, the expectation value $E[h]$ of a function $h(x)$ considering the distribution $f(x)$ ($P(x_i)$) of $x$ is defined as
        \begin{equation*}
          E[h] = \int_\chi h(x)f(x) \deriv{x}
          \quad\mathrm{or}\quad
          E[h] = \sum_{x_i\in\chi} h(x_i)P(x_i)
        \end{equation*}
      \item Another important summary statistic is the variance
        \begin{equation*}
          \Var[x] = \int_\chi (x-\langle x\rangle)^2 f(x) \deriv{x}
          \quad\mathrm{or}\quad
          \Var[x] = \sum_{x_i\in\chi} (x_i - \langle x\rangle)^2 P(x_i)
        \end{equation*}
        often used to characterize the width of a distribution
        \begin{itemize}
          \item The standard deviation $\sigma_x = \sqrt{\Var[x]}$ is also often used, because it has the same dimension as the random variable itself
        \end{itemize}
    \end{itemize}
  \end{frame}
% </Mean and Variance>

% <Covariance and Correlation>
  \begin{frame}{Covariance and Correlation}
    \begin{itemize}
      \item In cases with more than two variables the covariance between to random variables $(x, y)$ is defined equivalently as
        \begin{align*}
          \Cov[x, y] &= \int_\chi (x-\langle x\rangle)(y-\langle y\rangle) f(x,y) \deriv{x, y}
          \quad\mathrm{or}\quad \\
          \Cov[x, y] &= \sum_{x_i,y_i\in\chi}(x_i - \langle x\rangle)(y_i - \langle y\rangle) P(x_i, y_i)
        \end{align*}
        with the joint distributions $f$ ($P$) of $x, y$
      \item To express the linear dependence of two random variables $x, y$ the correlation is used
      \begin{equation*}
        \rho_{x,y} = \frac{\Cov[x, y]}{\Var[x]\Var[y]}
      \end{equation*}
      which is the normalized covariance in the interval $[-1, +1]$
      \begin{itemize}
        \item When $\Cov[x,y] = 0$, $x,y$ are statistically independent leading to $f(x,y) = f(x)f(y)$. The reverse is not automatically given though
      \end{itemize}
    \end{itemize}
  \end{frame}
% </Covariance and Correlation>

% <Bayes Rule>
  \begin{frame}{Bayes Rule}
    \begin{itemize}
      \item We can marginalize a multidimensional distribution by integrating (or summing) over all but one variable
        \begin{equation*}
          f(x) = \int_\mathcal{Y}f(x,y)\deriv{y}
          \quad\mathrm{or}\quad
          P(x) = \sum_{y_i\in\mathcal{Y}} P(x_i, y_i)
        \end{equation*}
        \item The following intuitive properties can be visualized and understood using Venn diagrams
          \begin{align*}
            P(A\,\mathrm{or}\,B) &= P(A) + P(B) - P(A\,\mathrm{and}\,B) \\
            P(A\,\mathrm{and}\,B) &= P(A) P(B|A) = P(B) P(A|B)
          \end{align*}
        \item From these the Bayes rule follows
          \begin{equation*}
            P(A|B) = \frac{P(B|A) P(A)}{P(B)}
          \end{equation*}
          or in the more general case with multiple disjunct events $A_i$ using marginalization of $P(B)$
          \begin{equation*}
            P(A_i|B) = \frac{P(B|A_i) P(A_i)}{\sum_{i=1}^N P(B|A_i)P(A_i)}
          \end{equation*}
    \end{itemize}
  \end{frame}
% </Bayes Rule>

% <Important Distributions>
  \begin{frame}{Important Distributions}
    \begin{itemize}
      \item Gaussian distribution in $d$ dimensions
        \begin{equation*}
          \mathcal{N}(\vec{x}|\vec{\mu},\underline{\Sigma})
          = \frac{\det{\underline{\Sigma}^{-1}}}{(2\pi)^{d/2}}
            \exp\left(
              -\frac{1}{2} (\vec{x}-\vec{\mu})^\mathrm{T}
              \underline{\Sigma}^{-1} (\vec{x}-\vec{\mu})
            \right)
        \end{equation*}
        with mean $\vec{\mu}$ and covariance matrix $\underline{\Sigma}$
      \item Binomial distribution
        \begin{equation*}
          P(k|n,p) = \begin{pmatrix} n \\ k \end{pmatrix} p^k (1-p)^{n-k}
          \quad\mathrm{for}\, k = 0, 1, \dots, n
        \end{equation*}
        describing the probability of getting $k$ successes in $n$ trials with a chance $p$ of getting a success in each trial
      \item Poisson distribution
      \begin{equation*}
        P(k|\lambda) = \frac{\lambda^k}{k!}\,e^{-\lambda}
      \end{equation*}
      describing the chance of counting $k$ hits within a defined time interval under a given expectation $\lambda$.\\
      It is a special case of the binomial distribution for small $p$ and many trials $n$
    \end{itemize}
  \end{frame}
% </Important Distributions>

% <Exercises>
  \begin{frame}{Exercises}
    \begin{exampleblock}{Exercise}
      \begin{enumerate}
        \item 1
      \end{enumerate}
    \end{exampleblock}
  \end{frame}
% </Exercises>

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MINIMIZERS
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \section{Minimization}
% <Analytical>
  \begin{frame}{Analytical}
    A
  \end{frame}
% </Analytical>

% <Bisection>
  \begin{frame}{Bisection}
    A
  \end{frame}
% </Bisection>

% <Newton Methods>
  \begin{frame}{Newton Methods}
    A
  \end{frame}
% </Newton Methods>

% <Gradient Descent>
  \begin{frame}{Gradient Descent}
    A
  \end{frame}
% </Gradient Descent>

% <Stochastic Gradient Descent>
  \begin{frame}{Stochastic Gradient Descent}
    A
  \end{frame}
% </Stochastic Gradient Descent>

% <Momentum Gradient Descent>
  \begin{frame}{Momentum Gradient Descent}
    A
  \end{frame}
% </Momentum Gradient Descent>

\end{document}
