\input{header}

% Main title, author etc. in header
\subtitle{Part II -- Machine Learning Basics\\\vspace*{1.5ex}
  \small \textbf{Goal:} Gain basic understanding on why we need machine learning, what the differences between the many algorithms is and get introduced to some popular approaches
}

\begin{document}
  \input{listing_setup}
  \maketitle

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Intro
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

  \begin{frame}{Fitting a line to data}
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{itemize}
          \item One of the most basic but also often needed questions: I have some data points, which line fits them best?
          \item This is a classical supervised regression learning algorithm (although it's usually not called this way)
          \item Can be solved using maximum likelihood parameter estimation
          \begin{itemize}
            \item In case you think more of least squares: LSQ can be obtained as a special case of likelihood fitting using a Gaussian model for the data points
          \end{itemize}
          \item An analytic solution exist for this exact problem by maximizing
          \begin{equation*}
            \argmax_{\theta}\mathcal{L} \propto \prod_i
              \frac{1}{\sqrt{2\pi}\sigma}
              \exp{-\frac{(y_i - f(x_i|\theta))^2}{2\sigma^2}}
          \end{equation*}
          where the parameters only appear linear in $f$
        \end{itemize}
      \end{column}

      \begin{column}{0.5\textwidth}
        \includegraphics[width=\textwidth]{02-img-fit_straight_line_example}
      \end{column}

    \end{columns}
  \end{frame}

  \begin{frame}{Maximum Likelihood estimation}
    \begin{itemize}
      \item In general the idea of maximum Likelihood estimation (MLE) is to tune the model parameters so that the data appears to be most likely to originate from the tuned model
      \item This is expressed by maximizing the expression
        \begin{equation*}
          \mathcal{L}(x|\theta) = \prod_{i=1}^N f(x_i | \theta)
        \end{equation*}
        which is just the product of the PDF value for each of the $i=1, \dots, N$ data points $x_i$.
      \item For numerical reasons, almost always the negative log-Likelihood is minimized instead
      \begin{equation}
        -\ln\mathcal{L}(x\theta) = -\sum_{i=1}^N \ln(f(x_i | \theta))
      \end{equation}
      \item Some simple LLH parameter estimates can be derived analytically, like the straight line fit before, most are too complicated and require numerical solutions
      \item Note: The model selection is crucial in MLE, it incorporates all the knowledge we have of the estimated process, for example physical conditions
      \item Depending on the model, MLE can be used for regression as well as classification tasks
    \end{itemize}
  \end{frame}

  \begin{frame}{Maximum Likelihood example plot}
    \includegraphics[width=\textwidth]{02-img-likelihood_estimation_example}
  \end{frame}

  \begin{frame}{Logistic Regression}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item In regression problems, like the line fit seen previously, it is tried to minimize the distance between the model and the data points
          \item However, in classification problems, data usually comes in discrete categories, also called classes
          \item These (binary) classes can be encoded with $Y\in {0, 1}$ (eg. sun will shine: $y=1$, sun won't shine: $y=0$), also called \enquote{one-hot-encoding}
          \item We can't simply fit a line to those data point, because the model would be meaningless
          \item \textbf{The way around: Logistic Regression}, which is, actually, a classification algorithm but uses regression to arrive there
          \item We use our Likelihood framework again, but we need to adapt our model to take the discrete classes into account
          % \item \emph{Note:} Regression and classification task are more or less the same, only with a difference in the choice of model.
          % Both operate within the Likelihood framework
        \end{itemize}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=\textwidth]{02-img-logit}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}{Logistic Regression}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item Using a modified Bernoulli LLH with a model of the success probability $p$ for a given $x_i$ rather than individual $p_i$ gives
          \begin{equation*}
            \mathcal{L} = \prod_i
              p(x_i | \theta)^{y_i} (1 − p(x_i | \theta))^{1 − y_i}
          \end{equation*}
          \item The most simple assumption for the model $p(x_i | \theta)$ is making it linear again, but now with a slight twist
          \begin{itemize}
            \item Because we want our model to predict between one and zero, not $p$ is assumed to be linear, but the expression $\ln{p / (1 - p)}$ (the \emph{logit} function)
            \item This seems arbitrary but, $p / (1 - p)$ are the odds and using the logarithm is a \enquote{natural} choice for extending the range to $[-\infty, \infty]$
          \end{itemize}
          \item We can then set
            \begin{equation}
              \ln \frac{p}{1 - p} = \theta x \Leftrightarrow
                p(x | \theta) = \frac{1}{1 + \exp(-\theta x)}
            \end{equation}
            which can be plugged into the Likelihood defined above
        \end{itemize}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=\textwidth]{02-img-logit}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}{Logistic Regression example plot}
    \includegraphics[width=\textwidth]{02-img-logistic_regression}
  \end{frame}

  \begin{frame}{Why do we need ML then?}
    \begin{columns}[c]
      \begin{column}{0.8\textwidth}
        \begin{itemize}
          \item With the maximum Likelihood formalism we have a very versatile and mighty tool at our hands
          \begin{itemize}
            \item It can be used to model regression and even classification tasks and estimate optimal parameters
            \item There is also a whole class of algorithms utilizing Bayes theorem to make posterior predictions (not covered here)
          \end{itemize}
          \item \textbf{So why do we need machine learning at all then?}
          \item For complex tasks, model building becomes equivalently complex
          \begin{itemize}
            \item How to write down a model for recognizing items in images?
              Potentially impossible to craft manually
            \item Model must be robust against rotation, translation, scaling, color variations, lightning variations, variations in the item itself, etc, etc ...
          \end{itemize}
          \item Big advantage of machine learning, aka the \enquote{learning}: Using an algorithm that \enquote{learns} the model itself by some kind of predefined measure (aka loss function) from a given dataset representing examples from the target distribution
          \begin{itemize}
            % \item Note: The term \enquote{learning} should not be understood to mean the same as human learning generally means.
            \item In general, the models are so general, that they adapt to many problems with very good performance
            \item In the end, it's \emph{just curve fitting}
          \end{itemize}
        \end{itemize}
      \end{column}
      \begin{column}{0.2\textwidth}
        \includegraphics[height=0.9\textheight]{02-img_cat_examples}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}{Different tasks to solve with ML}
    Different task in which machine learning algorithms are used (not complete):
    \begin{description}
      \item [Classic classiﬁcation] Sorting inputs in previously specified $k$ classes.
        Typically resembles a function $\mathbb{R}^n \rightarrow \{1, \dots, k\}$
      \item [Classic regression] The other classical task, predicting a continuous value from the input, typically $\mathbb{R}^n \rightarrow \mathbb{R}$
      \item [Transcription] Converting some kind of rather unstructured input (voice, text in image, etc.) in textual form (heavily used in modern speech recognition)
      \item [Translation] Translating between languages.
        Set in the field of natural language processing.
      \item [Anomaly detection] Noticing any kind of unusual behaviour or outliers in input data
      \item [Synthesis] Generating new data that looks similar to previously shown data.
        Eg. \emph{Deep Fake} or speech synthesis.
      \item [Filling of missing values] Filling \enquote{holes} in input data.
        Can be used to patch images with missing data or similar tasks
      \item [Denoising] Reducing noise in input data, for example in super resolution or also speech recognition
    \end{description}
  \end{frame}

  \begin{frame}{Supervised / unsupervised learners}
    \begin{columns}[t]
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          % \item Machine learning algorithms can roughly be separated into \emph{supervised} and \emph{unsupervised} algorithms
          \item \textbf{Supervised algorithms} are given a training data set with known labels
          \begin{itemize}
            \item In each learning step, the algorithm can compare its results to the ground truth, adapt and get better over time
            \item Popular example algorithms are: Naive Bayes, k-nearest neighbour, support vector machines, tree based learners, Neural Networks, linear discriminants and again linear regression
          \end{itemize}
          \item \textbf{Unsupervised algorithms} aim to work on datasets with no class information available  % and try to reduce the need for human oversight to a minimum
          \begin{itemize}
            \item Not being able to learn from a ground truth, such models usually try to find a representation of the underlying data distribution.
            \item Clustering algorithms, like k-means, try to group data points based on some distance measure
              % Cross validation can be used to estimate hyper-parameters like the number of clusters to find
            \item Neural network autoencoders try to find a compressed version of the input by piping the data through a bottleneck and then reconstructing them from that reduced input.
              % They can thus compare the input to their own generated output
            \item \emph{Expectation-maximization} is an iterative Likelihood method and eg. used in Gaussian mixture models
          \end{itemize}
          % \item Here focus mostly on an overview of supervised learners
        \end{itemize}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace*{1em}
        \includegraphics[width=\textwidth]{02-img-supervised_unsupervised_comic}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}{Info graphic supervised}
      \includegraphics[width=\textwidth]{02-img-how-do-machines-learn_supervised}
      \begin{center}
        \tiny[\url{https://www.boozallen.com/content/dam/boozallen_site/sig/pdf/infographic/how-do-machines-learn.pdf}]
      \end{center}
  \end{frame}

  \begin{frame}{Info graphic unsupervised}
      \includegraphics[width=\textwidth]{02-img-how-do-machines-learn_unsupervised}
      \begin{center}
        \tiny[\url{https://www.boozallen.com/content/dam/boozallen_site/sig/pdf/infographic/how-do-machines-learn.pdf}]
      \end{center}
  \end{frame}

  \begin{frame}{Reinforcement learning}
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{itemize}
          \item There is a third class of machine learning type, called \emph{reinforcement learning}
          \begin{itemize}
            \item Actually the borders are not always very clear and techniques and algorithms can end up blending concepts from multiple areas, and areas like \emph{semi-supervised learning} emerge
          \end{itemize}
          \item In reinforcement learning, the algorithm (\enquote{agent}) is presented with a reward of its actions in a given \enquote{environment} (simulation, real world task, etc.)
          \item The learner tries to tune the agent parameters so it gets a higher and higher reward when acting in the environment
          \item This field is often researched in the video game or simulation realm because new experiments can be done fast
          \begin{itemize}
            \item \url{https://deepmind.com/blog}
          \end{itemize}
        \end{itemize}
      \end{column}
      \begin{column}{0.5\textwidth}
        \includegraphics[width=\textwidth]{02-img-reinforcement}
        \begin{center}
          \tiny[\url{https://www.linkedin.com/pulse/machine-learning-explained-understanding-supervised-ronald-van-loon}]
        \end{center}
      \end{column}
    \end{columns}
  \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basics
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \section{Basics}

    \begin{frame}{Performance measures}
      \begin{columns}
        \begin{column}{0.65\textwidth}
          \begin{itemize}
            \item In machine learning algorithms, the definition of the loss function is the key task to define the behaviour and the capabilities of the learned system
            \item Defining the loss can get quite complicated to find the desired result
            \item For classification tasks, usually a simple \emph{accuracy} measure is used, which is just the proportion of correctly classified samples and usually encoded using the \emph{cross-entropy loss}
            \item For regression tasks, the mean squared error is often used to measure the deviation of the model output to the data point for each sample.
              Many variants exists, for example for regularization (Huber loss)
            \item For example in a style transfer generative model, the loss is a combination from a \emph{style loss} and \emph{content loss} to produce the desired output.
          \end{itemize}
        \end{column}
        \begin{column}{0.35\textwidth}
          \vspace*{1em}
          \includegraphics[width=\textwidth]{02-img-loss_functions}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Other classifier performance metrics}
      \enlargethispage{1cm}
      \begin{columns}
        \begin{column}{0.65\textwidth}
          \begin{itemize}
            % \item For classifiers, there are other performance measures besides the value of the loss as in the regression case
            \item We can define \enquote{true positives / negatives} (TP / NP) and \enquote{false positives and negatives} (FP / NP) and show them in a truth table (\emph{confusion matrix})
            \item From these we can define some performance metrics, like
            \begin{itemize}
              \item Sensitivity: $\text{TP} / (\text{TP} + \text{FN})$
              \item Precision: $\text{TP} / (\text{TP} + \text{FP})$
              \item Fall-out: $\text{FP} / (\text{FP} + \text{TN})$
              \item Accuracy: $(\text{TP} + \text{TN}) / N_\text{data}$
              \item Note: When classes are imbalanced, these are pretty misleading (eg. $99\%$ of examples are class 0, then it's trivial to achieve $99\%$ accuracy)
            \end{itemize}
            \item Often, the area under \emph{ROC curves} is also used
            \begin{itemize}
              \item After classification, move decision boundary from $0$ to $1$ and plot fall-out against sensitivity
              \item This gives a curve from $(0,0)$ to $(1, 1)$
              \item We want that curve to have a high sensitivity even for low fall-outs, so the area should be close to one (curve should bend towards top left corner)
              \item A guessing classifier is represented by the diagonal line
            \end{itemize}
          \end{itemize}
        \end{column}
        \begin{column}{0.35\textwidth}
          \centering
          \begin{tabular}{cccc}
            & & \multicolumn{2}{c}{\small\textit{Predicted}} \\
            & & True & False \\
           \multirow{2}{*}{\STAB{\rotatebox[origin=c]{90}{
            \small\textit{Truth} }}}
            & True & \cellcolor{nordAuroraGreen}\textcolor{white}{TP} &
              \cellcolor{nordAuroraRed}\textcolor{white}{FN} \\
            & False & \cellcolor{nordAuroraRed}\textcolor{white}{FP} &
              \cellcolor{nordAuroraGreen}\textcolor{white}{TN} \\
          \end{tabular}

          \vspace{1cm}
          \includegraphics[width=\textwidth]{02-img-roc_curve}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Loss functions - Regression}
      \enlargethispage{1cm}
      There are few frequently used loss functions for standard regression and classification task
      \begin{description}
        \item[Mean squared error (MSE)] is the standard regression loss function
          \begin{equation*}
            \frac{1}{N} \sum_{i=1}^N \left(y_i - \hat{y}_i\right)^2
            \minter{or weighted}
            \frac{1}{N} \sum_{i=1}^N w_i\left(y_i - \hat{y}_i\right)^2
            \minter{where usually}
            w_i = \frac{1}{\sigma_i^2}
          \end{equation*}
          \begin{itemize}
            \item Penalizes large deviation from the true values quadratically
            \item Has troubles with outlier points as they tend to draw all the attention to themselves
          \end{itemize}
        \item[Mean absolute error (MAE)] is the standard regression loss function
          \begin{equation*}
            \frac{1}{N} \sum_{i=1}^N \left|y_i - \hat{y}_i\right|
          \end{equation*}
          \begin{itemize}
            \item Large deviations from the true values are only penalized linearly, more robust to outliers
            \item The \emph{Huber loss} combines MSE and MAE by switching continuously at some $\delta$
          \end{itemize}
      \end{description}
      Note: This list is not complete, see \url{https://keras.io/api/losses/} for a nice overview
    \end{frame}

    \begin{frame}{Regression loss functions example}
        \includegraphics[width=\textwidth]{02-img-regression_losses}
    \end{frame}

    \begin{frame}{Loss functions - Classification}
      \enlargethispage{1cm}
      There are few frequently used loss functions for standard regression and classification task
      \begin{description}
        \item[Binary Cross Entropy (BCE)] is the standard classification loss function for two classes
          \begin{equation*}
            - \frac{1}{N} \sum_{i=1}^N \left(
              y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
          \end{equation*}
          \begin{itemize}
            \item $y_i$ are the true, \emph{one-hot-encoded} labels, so for a point belonging to the first class, $y_i = 1$, so only the first log term survives
            \item Has troubles with outlier points as they tend to draw all the attention to themselves
          \end{itemize}
        \item[Categorial Cross Entropy] is the same thing as the BCE above, but for more than two classes.
        It generalises for $C$ classes as
          \begin{equation*}
            - \frac{1}{N}\sum_{i=1}^N \sum_{j=1}^C y_{ij} \log(\hat{y}_{ij})
          \end{equation*}
          \begin{itemize}
            \item For two classes we retrieve the BCE estimator
            \item This is also used one-hot-encoded, eg. for $4$ classes, the truth vector for a class $3$ example would be $\vec{y}_i = [0, 0, 1, 0]$
          \end{itemize}
      \end{description}
      Note: This list is not complete, see \url{https://keras.io/api/losses/} for a nice overview or \url{https://gombru.github.io/2018/05/23/cross_entropy_loss/} for a more in-depth explanation
    \end{frame}

    \begin{frame}{Overfitting / underfitting}
      \begin{itemize}
        \item A key challenge in machine learning applications is that the focus lies more on a broad validity of the model rather than trying to get the maximum information from the training set
        \item This means the trained model should also behave well on previously unseen data, not used in training
        % \item Compare: In statistic we're usually more interested in constraining some set of true parameters with the available data and model as good as possible
        \item To be able to check the performance on unseen data, a piece of the whole available data is split of and not used for training but later for checking the trained model
        \item We then have two factors that determine how well our model performs
      \end{itemize}
      \begin{description}
        \item[Underfitting] is usually meant when the model does not yield a low enough loss on training data
        \begin{itemize}
          \item This usually happens, when the model is not complex or flexible enough to capture the shape of the data distribution
          \item We say the model has a high \emph{bias} (bias regarding the   inflexibility of the model)
        \end{itemize}
        \item[Overfitting] means we have a really good performance on training data, but on unseen data, the performance is much worse
          \begin{itemize}
            \item We get overfitting, when we use a too complex model which adapts perfectly to training data, but then misses all the training data points
          \item We say the model has a high \emph{variance} (performance highly fluctuates on unseen data)
          \end{itemize}
      \end{description}
    \end{frame}

    \begin{frame}{Over- and underfitting example}
      \includegraphics[width=\textwidth]{02-img-over_underfit}
      \enlargethispage{3em}
      % \vspace{-1em}
      \emph{\footnotesize Note: For regression, line fit, polygon of 2nd and 11th order; for classification, logistic regression, plain neural net, over-fitter plain neural net without regularization and too large layers}
    \end{frame}

    \begin{frame}{Regularization}
      \begin{itemize}
        \item Consider solving a system of linear equations, if there are more variables than equation, then there is no solution to the problem
        \begin{itemize}
          \item But if we introduce artificial constraints on the parameters, we can find a solution
          \item The cost for introducing the constraints is an additional bias in the model
        \end{itemize}
        \item For a general loss function, the regularization term is simply added to the loss
          \begin{equation*}
            \frac{1}{N}\sum_{i=1}^N \mathrm{loss}_i \rightarrow
            \frac{1}{N}\sum_{i=1}^N \mathrm{loss}_i(\theta)
              + \lambda \sum_j\mathrm{reg}(\theta_j)
          \end{equation*}
          wehre $\theta$ is the regularization strength as a tunable parameter (if $0$, then we won't have regularization)
        \item Popular regularization terms are
        \begin{itemize}
          \item \textbf{L2 (\emph{ridge} or \emph{Tikhonov})} regularization: $\mathrm{reg} = \sum_j \theta_j^2$; yields parameters that are as small as possible
          \item \textbf{L1 (\emph{lasso})} regularization: $\mathrm{reg} = \sum_j\left|\theta_j\right|$; yields a sparse parameter set
        \end{itemize}
        \item There are also other \enquote{effective} regularization techniques:  %, that try to minimize variance by averaging over randomized ensembles of models
        \begin{description}
          \item[Dropout] is often used in neural networks; in each training step, certain parts of the network are \enquote{turned off} simulating an average of multiple models
          \item[Bagging] is similar to dropout and often used for tree based learners; instead of turning of certain parameters, many models are trained with only a randomly sampled fraction of the data and then averaged
        \end{description}
      \end{itemize}
    \end{frame}

    \begin{frame}{Validation}
      \begin{columns}
        \begin{column}{0.7\textwidth}
          \begin{itemize}
            \item To properly validate a trained model, we need to know how well it generalizes on unseen data
            \item \textbf{Note: Using all available data for training and quoting the training performance is a very bad idea}
            \item ALWAYS split your data in a training and evaluation / test set
            \item The most convenient way to evaluate the model performance is \emph{cross validation}
            \begin{itemize}
              \item Instead of splitting into only one training and test set, we split into $k$ equally sized but randomly sampled subset (k-fold cross validation)
              \item Somehow $k=10$ is often used; If only a single element is used for testing, it is called \emph{leave-one-out validation}
              \item So we split our data in $k$ subsets and train $k$ models on each training set, where a training set consists of $k-1$ subsets, successively leaving out every test set over all trainings
              \item Then we test the model on the left out test sample in each iteration
              \item In the end we can average the performance of each $k$th iteration and get a proper idea how well the model behaves on average on unseen data and how robust it is under a changing training set
            \end{itemize}
          \end{itemize}
        \end{column}
        \begin{column}{0.3\textwidth}
          \includegraphics[width=\textwidth]{02-img-cross_validation_data_split}

          \vspace{1cm}

          \includegraphics[width=\textwidth]{02-img-cv_roc_curve}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Hyperparamter search / tuning}
      \enlargethispage{1cm}
      \begin{columns}
        \begin{column}{0.75\textwidth}
          \begin{itemize}
            \item \emph{Hyperparameters} are tunable values, that are not related to the weights of the learner but rather to the algorithm itself
            \begin{itemize}
              \item Eg. number of trees in a random forest, number of clusters, learning rates, regularization strengths, $\dots$
            \end{itemize}
            \item To find an optimal set of these, we usually simply have to try them out
            \item Systematic approaches sample parameters on some kind of grid and compare the performances to select the best one
            \begin{description}
              \item[Grid search] is the classic approach of trying parameters on a rectangular grid; Drawbacks are, that the underlying distributions are only sparsely samples
              \item[Random search] distributes the parameter combinations randomly; The underlying distributions are better sampled then
            \end{description}
            \item Additionally, meta minimizers, evolutionary algorithms or other sophisticated methods can be used, to descent to a minimum parameter set more quickly and robustly
          \end{itemize}
        \end{column}
        \begin{column}{0.25\textwidth}
          \vspace*{1em}

          \includegraphics[width=\textwidth]{02-img-hyperparameter_search}
        \end{column}
      \end{columns}
      \footnotesize Usually we let some packages do the work for us, eg. \url{http://hyperopt.github.io/hyperopt/} or \url{https://scikit-learn.org/stable/modules/grid_search.html}
    \end{frame}

    % \begin{frame}{Feature extraction / generation}
    %   \begin{itemize}
    %     \item PCA -> Also shortly mention as a unsupervised learner
    %     \item Generative algorithms
    %     \item MRMR
    %     \item Forward / backward selection
    %   \end{itemize}
    % \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example ALgoithms
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithms}
  \begin{frame}{Intro}
    \begin{itemize}
      \item On the following slides, each slide introduces a learning algorithm to get a good overview of available techniques
      \item Below are some links to get further information for the algorithms presented here and many more
      \begin{itemize}
        \item \url{https://scikit-learn.org/stable/user_guide.html}
        \item \url{http://spark.apache.org/docs/latest/ml-guide.html}
        \item \url{https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/}
        \item \url{https://towardsdatascience.com/top-10-machine-learning-algorithms-for-data-science-cdb0400a25f9}
        \item \url{https://en.wikipedia.org/wiki/Supervised_learning}
        \item \url{https://en.wikipedia.org/wiki/Unsupervised_learning}
        \item \url{https://en.wikipedia.org/wiki/Reinforcement_learning}
      \end{itemize}
    \end{itemize}
  \end{frame}

  \subsection{Supervised}

  \begin{frame}[fragile]{Manual Likelihood fit}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item We already saw examples of Likelihood fitting
          \item Usually this technique is used more often in statistics when there is need for a robust error estimation of the estimated parameters
          \item How to:
          \begin{enumerate}
            \item Define your Likelihood model suitable for your data
            \item Minimize the negative log-Likelihood (numerically) and yield the best parameter set
          \end{enumerate}
        \end{itemize}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{Gaussian LLH fit [Note: example shortened]}]
            # Note: x is the data array, previously defined
            def neglogllh(params):
                mean, stddev = params
                return -np.sum(scipy.stars.norm.logpdf(
                    x, loc=mean, scale=stddev))

            theta0 = [0, 1]  # Seed: Standard normal distribution
            best_fit = scipy.optimize.minimize(neglogllh, theta0)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_llh}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{k Nearest Neighbours (kNN) classifier}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item \emph{kNN} is a supervised classification algorithm
          \item It does not have a \enquote{real} training step, it simply stores all training data
          \item On prediction, the new point is compared to all neighbours and the majority vote of the $k$ nearest neighbour points wins
          \item The distance metric can be adopted to the needs, eg. euclidean or taxi or cosine distance, etc.
          \item \small\url{https://scikit-learn.org/stable/modules/neighbors.html#classification}
        \end{itemize}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{kNN classification [Note: example shortened]}]
            from sklearn.neighbors import KNeighborsClassifier
            knn = KNeighborsClassifier(n_neighbors=3)
            knn.fit(data_train, classes)
            preds = knn.predict_proba(data_test)[:, 1]
            preds_classes = knn.predict(data_test)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_knn}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{LDA Classification}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item \emph{LDA} (linear discriminant analysis) is a supervised, linear classification algorithm
          \item It tries to find the separating hyperplane in $N_\text{feat} - 1$ dimension that maximizes the distance between the class means and minimizes the projected variances per class
          \item In 1D the separator is a straight line
          \item The loss is
            $(\vec{\mu}'_1 - \vec{\mu}'_2)^2(\Sigma'_1 + \Sigma'_2)^{-1}$
            where $\vec{\mu}'_i$ is the projected mean and $\Sigma'_i$ the proj. covariance of class $i$
          \item \small\url{https://scikit-learn.org/stable/modules/lda_qda.html#lda-qda}
        \end{itemize}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{LDA classification [Note: example shortened]}]
            from sklearn.discriminant_analysis import \
                LinearDiscriminantAnalysis
            lda = LinearDiscriminantAnalysis()
            lda.fit(data_train, classes)
            preds = lda.predict_proba(data_test)[:, 1]
            preds_classes = lda.predict(data_test)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_lda}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{Support vector machine classifier}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          % \item \emph{SVM} (support vector machine) is a supervised, classification algorithm
          \item \emph{SVM} works similar to the LDA by trying to find a separating hyper plane, but here, the algorithm tries place the plane so that a margin region between the classes becomes as large as possible
          % \item However, the loss is defined a bit different (\emph{Hinge loss}) which yields a \enquote{maximum margin classification}
          \item SVM can be made very flexible by using transformation functions (search for \enquote{kernel trick}) to transform the data on-the-fly to a higher dimensional space, where a linear separator can be found
          \item \small\url{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}
        \end{itemize}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{SVM classification [Note: example shortened]}]
            from sklearn.svm import SVC
            svm = SVC(kernel="rbf")  # Use a non-lin kernel
            # svm = SVC(kernel="linear")  # Or a linear kernel
            svm.fit(data_train, classes)
            preds_classes = svm.predict(data_test)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_svm}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{Naive Bayes classifier}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          % \item \emph{SVM} (support vector machine) is a supervised, classification algorithm
          \item The \emph{Naive Bayes} classifier uses the Bayes theorem $P(A|B) = P(B|A)P(A)/P(B)$ in combination to an assumption of the underlying class distribution to make predictions
          \item The needed numerical values for $P(B|A),P(A),P(B)$ are constructed from counting the combinations for categorical data or using eg. a Gaussian LLH for real valued features
          \item \small\url{https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes}.
            You need to select the proper one for your assumption on the data distribution.
          \item Note: The \enquote{naive} means that we assume each feature is independent.
            That's the reason why it can't perfectly separate the Gaussian with covariances in the lower right plot
        \end{itemize}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{Naive Bayes classification [Note: example shortened]}]
            from sklearn.naive_bayes import GaussianNB
            naiveb = GaussianNB()  # Also used in the plots
            naiveb.fit(data_train, classes)
            preds_classes = naiveb.predict(data_test)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_naive_bayes}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{Decision Tree - Classification}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item \emph{Tree} based classifiers, like a simple \emph{Decision Tree} or a more sophisticated \emph{Random Forest} build a series of straight cuts in a tree based order during training
          \item Cut selection is based on \emph{entropy gain} (see logistic regression) or the \emph{Gini-Index}
          \item We can see the straight cuts directly in the plots on the right: The decision boundary is put together from a series of axes-parallel lines
          \item Note: Simple trees almost always require some kind of regularization, as they tend to overfit a lot
          \item \small\url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html}
        \end{itemize}
        \vspace{-0.5em}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{Decision Tree classification [Note: example shortened]}]
            from sklearn.tree import DecisionTreeClassifier
            tree = DecisionTreeClassifier()
            tree.fit(data_train, classes)
            preds_classes = tree.predict(data_test)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_tree_classification}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{Decision Tree - Regression}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item \emph{Tree learners} can also be used as regressors, we simply have to exchange the split decision loss to a mean squared error or others we saw before
          \item On each split, the cut is chosen, which minimizes the distance in the leaf to a straight line segment
          \item Therefore, a tree based regressor also produces straight line segments as the regression and not a continuous curve
          \item \small\url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}
        \end{itemize}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{Decision Tree regression [Note: example shortened]}]
            from sklearn.tree import DecisionTreeRegressor
            tree = DecisionTreeRegressor()
            tree.fit(x_vals, y_vals)
            preds_classes = tree.predict(x_test_vals)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_tree_regression}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{Clustering}
    \begin{columns}
      \begin{column}{0.65\textwidth}
        \begin{itemize}
          \item \emph{k-Means} is an iterative, unsupervised clustering algorithm, that means it does not (need to) know about the true labels
          \item The k-Means algorithm tries to minimize the distance from the $k$ class centres to each point belonging to that class $\sum_{i=1}^k \sum_{x_j\in C_i} (x_j - \mu_i)^2$
          \item The number of clusters is a fixed hyper parameter
          \item The iteration usually works as follows (iterate 2-3)
          \begin{enumerate}
            \item Init cluster centers (usually random)
            \item Assign each point to the cluster which has the lowest increase in variance
            \item Re-calculate the cluster centres by averaging the points
          \end{enumerate}
          \item \small\url{https://scikit-learn.org/stable/modules/clustering.html#k-means}
        \end{itemize}
        \vspace{-1em}
        \begin{mdframed}
          \begin{lstlisting}[style=dark, gobble=10, title=\lsttitlelight{Decision Tree regression [Note: example shortened]}]
            from sklearn.cluster import KMeans
            kmeans = kMeans(n_clusters=2)
            kmeans.fit(data_train)  # No labels needed
            preds_classes = kmeans.predict(data_test)
          \end{lstlisting}
        \end{mdframed}
      \end{column}
      \begin{column}{0.35\textwidth}
        \vspace{1em}

        \includegraphics[width=0.9\textwidth]{02-img-algos_kmeans}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[fragile]{Simple Fully-Connected Neural Network Classifier}
  Classification
  \end{frame}

  \begin{frame}[fragile]{Simple Fully-Connected Neural Network Regressor}
  Regression
  \end{frame}



\end{document}
